# Course 3: Natural Language Processing in TensorFlow

In this third course, you’ll learn how to apply neural networks to solve natural language processing problems using TensorFlow. You’ll learn how to process and represent text through tokenization so that it’s recognizable by a neural network. You’ll be introduced to new types of neural networks, including RNNs, GRUs and LSTMs, and how you can train them to understand the meaning of text. 


## Week 1: Sentiment in Text
- Introduction: A conversation with Andrew Ng
- Word-based encodings
- Using APIs
- Text to sequence
- Sarcasm, really?
- Working with the Tokenizer
- Week 1.1 - Detecting sarcasm in news headlines with LSTM and CNN.ipynb
- Week 1.2 - Exploring BBC news data.ipynb

## Week 2: Word Embeddings
- A conversation with Andrew Ng
- The IMDB dataset
- Looking into the details
- How can we use vectors?
- More into the details
- Remember the sarcasm dataset?
- Building a classifier for the sarcasm dataset
- Let’s talk about the loss function
- Pre-tokenized datasets
- Diving into the code
- Week 2.1 - Classifying IMDB reviews data (Embedding + MLP).ipynb
- Week 2.2 - Classifying BBC news into topics (Embedding + Conv + MLP).ipynb

## Week 3: Sequence Models

- A conversation with Andrew Ng
- LSTMs
- Implementing LSTMs in code
- A word from Laurence
- Accuracy and Loss
- Using a convolutional network
- Going back to the IMDB dataset
- Tips from Laurence
- Week 3.1 - Classifying IMDB reviews (Embedding + Conv1D).ipynb
- Week 3.2 - Twitter sentiment classification (GloVe).ipynb

## Week 4: Sequence Models and Literature
- A conversation with Andrew Ng
- Training the data
- Finding what the next word should be
- Predicting a word
- Poetry!
- Laurence the poet
- Week 4 - Poem generation with Bi-directional LSTM.ipynb
