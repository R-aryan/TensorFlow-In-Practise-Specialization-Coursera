# Course 3: Natural Language Processing in TensorFlow

In this third course, you’ll learn how to apply neural networks to solve natural language processing problems using TensorFlow. You’ll learn how to process and represent text through tokenization so that it’s recognizable by a neural network. You’ll be introduced to new types of neural networks, including RNNs, GRUs and LSTMs, and how you can train them to understand the meaning of text. 

- My Certificate For This Course- [Click Here](https://coursera.org/share/0794a0f007e5407e716ab42066e8957f)


## Week 1: Sentiment in Text
- Introduction: A conversation with Andrew Ng
- Word-based encodings
- Using APIs
- Text to sequence
- Sarcasm, really?
- Working with the Tokenizer


## Week 2: Word Embeddings
- A conversation with Andrew Ng
- The IMDB dataset
- Looking into the details
- How can we use vectors?
- More into the details
- Remember the sarcasm dataset?
- Building a classifier for the sarcasm dataset
- Let’s talk about the loss function
- Pre-tokenized datasets
- Diving into the code


## Week 3: Sequence Models

- A conversation with Andrew Ng
- LSTMs
- Implementing LSTMs in code
- A word from Laurence
- Accuracy and Loss
- Using a convolutional network
- Going back to the IMDB dataset
- Tips from Laurence


## Week 4: Sequence Models and Literature
- A conversation with Andrew Ng
- Training the data
- Finding what the next word should be
- Predicting a word
- Poetry!
- Laurence the poet

